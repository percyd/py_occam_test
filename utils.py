# py_occam/utils.py

import pandas as pd
import numpy as np
from typing import List, Dict, Optional, Tuple
import yaml
from pathlib import Path

def load_data(tsv_path: str, yaml_path: Optional[str] = None,
             columns: Optional[List[str]] = None) -> Tuple[pd.DataFrame, Dict]:
    """
    Load data and variable definitions from TSV and YAML files.
    
    Args:
        tsv_path: Path to TSV data file
        yaml_path: Optional path to YAML sidecar. If None, assumes same name as TSV with .yml
        columns: Optional list of columns to keep. If None, keeps all columns.
        
    Returns:
        Tuple of (DataFrame, YAML config dict)
    """
    # Load TSV data
    data = pd.read_csv(tsv_path, sep='\t')
    
    # Filter columns if specified
    if columns:
        data = data[columns]
        
    # Get YAML path if not specified
    if yaml_path is None:
        yaml_path = str(Path(tsv_path).with_suffix('.yml'))
        
    # Load YAML config
    with open(yaml_path) as f:
        config = yaml.safe_load(f)
        
    return data, config

def create_yaml_sidecar(data: pd.DataFrame, 
                       dv_col: str,
                       output_path: str,
                       ignore_cols: Optional[List[str]] = None,
                       abbrevs: Optional[Dict[str, str]] = None) -> None:
    """
    Create YAML sidecar file from DataFrame.
    
    Args:
        data: Input DataFrame
        dv_col: Name of DV column
        output_path: Path to write YAML file
        ignore_cols: Optional list of columns to mark as type=0
        abbrevs: Optional dict mapping column names to abbreviations
    """
    if ignore_cols is None:
        ignore_cols = []
        
    if abbrevs is None:
        abbrevs = {}
        
    # Create variables section
    variables = {}
    for col in data.columns:
        # Generate abbreviation
        if col in abbrevs:
            abbrev = abbrevs[col]
        else:
            abbrev = col[0].lower()  # Default to first letter
            
        # Determine variable type
        if col in ignore_cols:
            var_type = 0
        elif col == dv_col:
            var_type = 2
        else:
            var_type = 1
            
        # Add variable definition
        variables[col] = {
            'abbrev': abbrev,
            'cardinality': data[col].nunique(),
            'type': var_type,
            'rebin': None
        }
        
    # Create YAML structure
    config = {
        'version': '1.0',
        'variables': variables,
        'metadata': {
            'description': 'Generated by py-occam',
            'dv_name': dv_col,
            'sample_size': len(data)
        }
    }
    
    # Write YAML file
    with open(output_path, 'w') as f:
        yaml.dump(config, f, sort_keys=False)
        
def validate_data(data: pd.DataFrame, config: Dict) -> None:
    """
    Validate data against YAML config.
    
    Args:
        data: DataFrame to validate
        config: YAML config dict
    
    Raises:
        ValueError: If validation fails
    """
    variables = config['variables']
    
    # Check all required columns present
    for var_name, var_def in variables.items():
        if var_def['type'] > 0:  # Not ignored
            if var_name not in data.columns:
                raise ValueError(f"Required column {var_name} not in data")
                
    # Check cardinalities match
    for var_name, var_def in variables.items():
        if var_name in data.columns:
            card = data[var_name].nunique()
            if card != var_def['cardinality']:
                raise ValueError(
                    f"Cardinality mismatch for {var_name}: "
                    f"expected {var_def['cardinality']}, got {card}"
                )
                
    # Verify single DV
    dv_cols = [name for name, var in variables.items() 
               if var['type'] == 2]
    if len(dv_cols) != 1:
        raise ValueError(f"Expected 1 DV, found {len(dv_cols)}")
        
def print_model_summary(models: List[Dict], sort_by: str = 'bic') -> None:
    """
    Print summary of models in standard format.
    
    Args:
        models: List of model dicts with statistics
        sort_by: Statistic to sort by
    """
    # Print header
    print("\nID MODEL Level H dDF dLR Alpha Inf %dH(DV) dAIC dBIC Inc.Alpha Prog. %C(Data) %cover")
    
    # Sort models
    sorted_models = sorted(models, 
                         key=lambda m: (-m['statistics'][sort_by], m['statistics']['h']))
    
    # Print each model
    for model in sorted_models:
        stats = model['statistics']
        print(f"{model['id']:2d}{'*' if model['is_loopless'] else ' '} "
              f"{model['name']:20s} "
              f"{model['level']:2d} "
              f"{stats['h']:8.4f} "
              f"{stats['ddf']:4d} "
              f"{stats['dlr']:8.4f} "
              f"{stats['alpha']:8.4f} "
              f"{stats['information']:8.4f} "
              f"{stats['dh_dv']:8.4f} "
              f"{stats['aic']:8.4f} "
              f"{stats['bic']:8.4f} "
              f"{model.get('incremental_alpha', 0.0):8.4f} "
              f"{model.get('progenitor_id', 0):4d} "
              f"{stats.get('pct_correct', 0.0):8.4f} "
              f"{100.0000:8.4f}")